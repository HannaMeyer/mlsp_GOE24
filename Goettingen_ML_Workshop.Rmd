---
title: "Introduction to machine learning for spatial mapping of the environment"
author: "Hanna Meyer"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
editor_options: 
  chunk_output_type: console
---




## Introduction

This tutorial provides the code for the workshop "Introduction to machine learning for spatial mapping of the environment" taught at the University of Göttingen.
Our aim is to produce a spatial continuous map of plant species richness for South America based on reference data from sPlotOpen, assuming that climate and elevation are predictors of species richness. We assume complex relationships, therefore we will use random forest as machine learning algorithm.

### How to start

To work with the tutorial, first load the required libraries. If not yet installed, install them via install.packages("packagename"):

```{r, message = FALSE, warning=FALSE}
#install.packages("sf")
library(sf) # for vector data handling
library(terra) # fot raster data handling
library(mapview) # for interactive visualizations
library(caret) # for machine learning 
library(CAST) # extension of caret for spatial data
```


## Description of the example dataset
We will work with data of the sPlotOpen vegetation database. A subset is already provided within the CAST package and can hence simply be loaded.

```{r, message = FALSE, warning=FALSE}
data(splotdata)
head(splotdata)
# additionally let's load our model domain (South America)
modeldomain <- st_read("data/modeldomain.gpkg")
```
The data includes species richness sampled at the plot locations and it includes climate and elevation as predictors. The predictors were derived from Worldclim.

To get an impression on the spatial properties of the dataset, let's have a look on the spatial distribution of the vegetation plots in South America:

```{r, message = FALSE, warning=FALSE}
plot(splotdata[,"Species_richness"])
```

```{r, message = FALSE, warning=FALSE}
#...or plot the data with mapview:
mapview(splotdata)
```


## First simple model training and prediction
To start with, lets use this dataset to create a "default" Random Forest model that predicts species richness based on some predictor variables.

```{r, message = FALSE, warning=FALSE}
predictors <- c("bio_1", "bio_4", "bio_5", "bio_6",
                "bio_8", "bio_9", "bio_12", "bio_13",
                "bio_14", "bio_15", "elev")

# to use the data for model training we have to get rid of the geometry column
trainDat <- st_drop_geometry(splotdata)

model <- train(trainDat[,predictors],
               trainDat$Species_richness,
               method="rf")
```


Based on the trained model we can make spatial predictions of species richness. To do this we load a multiband raster that contains spatial data of all predictor variables for entire South America (identical to the data already extracted for the reference locations). We then apply the trained model on this data set.


```{r, message = FALSE, warning=FALSE}
predictors_sp <- rast("data/predictors.tif")
prediction <- predict(predictors_sp,model,na.rm=TRUE)
plot(prediction)
```

The result is a spatially comprehensive map of the species richness of South America.
We see that simply creating a map using machine learning and caret is an easy task, however accurately measuring its performance is less simple. Though the map looks good on a first sight we now have to follow up with the question of how accurate this map is, hence we need to ask how well the model is able to map species richness.


## Validation

### Assessing the training error
Well a first idea might be to simply compare the predictions with the reference.

```{r, message = FALSE, warning=FALSE}
pred_extract <- extract(prediction,splotdata)

plot(pred_extract$lyr1,splotdata$Species_richness)
validation_model <- lm(splotdata$Species_richness~pred_extract$lyr1)
abline(validation_model,col="red")
summary(validation_model)
```

But is this a good way of model validation? Certainly not! A complex model can reproduce the training data but that doesn't tell us the ability of the model to make predictions for new areas.
Instead, we should split the data into training and test data, or, if the data set is too small validate the model via cross-validation, which additionally serves the purpose of model tuning.


### Cross validation
Among validation strategies, k-fold cross validation (CV) is popular to estimate the performance of the model in view to data that have not been used for model training. During CV, models are repeatedly trained (k models) and in each model run, the data of one fold are put to the side and are not used for model training but for model validation. In this way, the performance of the model can be estimated using data that have not been included in the model training.

#### The Standard approach: Random k-fold CV

Let's use a default random k-fold CV. Hence, the data points in our dataset are RANDOMLY split into k folds. Let's set k to 5 here.


```{r, message = FALSE, warning=FALSE}

ctrl <- trainControl(method="cv",
             number=5,
             savePredictions="final")

model <- train(trainDat[,predictors],
               trainDat$Species_richness,
               method="rf",
               trControl = ctrl)
```


To assess the performance of the model let's have a look on the output of the Random CV:


```{r, message = FALSE, warning=FALSE}
global_validation(model)
plot(model$pred$pred,model$pred$obs)
```


Unfortunately, the random k fold CV still does not give us a good indication for the map accuracy. Random k-fold CV means that each of the folds (with the highest certainty) contains data points from each known location. Therefore, a random CV cannot indicate the ability of the model to make predictions beyond the location of the training data (i.e. to map species richness). Since our aim is to map species richness, we rather need to perform a target-oriented validation which validates the model in view to spatial mapping.


#### Spatial cross-validation

We are not interested in the model performance in view to random subsets of our vegetation plots, but we need to know how well the model is able to make predictions for areas without reference samples.
To find this out, we need to repeatedly leave larger spatial regions of one or more vegetation plots out and use them as test data during CV. Let's use the knndm approach for this


```{r, message = FALSE, warning=FALSE}

knndm_folds = knndm(tpoints = splotdata,
                    modeldomain = modeldomain, 
                    k = 5)

ctrl_knndm <- trainControl(method="cv",
                     index = knndm_folds$indx_train,
                     indexOut = knndm_folds$indx_test,
                     savePredictions = "final")

model <- train(trainDat[,predictors],
                   trainDat$Species_richness,
                   method="rf",
                   trControl=ctrl_knndm)
global_validation(model)
plot(model$pred$pred,model$pred$obs)
```


By inspecting the output of the model, we see that in view to new locations, the R² is much lower than what was expected from the random CV.

Apparently, there is considerable overfitting in the model, causing a good random performance but a poor performance in view to new locations. This might partly be attributed to the choice of variables where we must suspect that certain variables are misinterpreted by the model (see [Meyer et al 2018](https://www.sciencedirect.com/science/article/pii/S1364815217310976) or [talk at the OpenGeoHub summer school 2019] (https://www.youtube.com/watch?v=mkHlmYEzsVQ)).

Let's have a look at the variable importance ranking of Random Forest.


```{r, message = FALSE, warning=FALSE}
plot(varImp(model))
```

Assuming that certain variables are misinterpreted by the algorithm we should be able to produce a higher spatial performance when such variables are removed. Let's see if this is true in the next section...


## Removing variables that cause overfitting
CAST's forward feature selection (ffs) selects variables that make sense in view to the selected CV method and excludes those which are counterproductive (or meaningless) in view to the selected CV method.
When we use a spatial CV method (here: knndm), ffs selects variables that lead in combination to the highest spatial performance (i.e. the best spatial model). All variables that have no spatial meaning or are even counterproductive won't improve or even reduce the spatial performance and are therefore excluded from the model by the ffs.

ffs is doing this job by first training models using all possible pairs of two predictor variables. The best model of these initial models is kept. On the basis of this best model the predictor variables are iterativly increased and each of the remaining variables is tested for its improvement of the currently best model. The process stops if none of the remaining variables increases the model performance when added to the current best model.

So let's run the ffs on our case study using R² as a metric to select the optimal variables. This process will take 1-2 minutes...

```{r, message = FALSE, warning=FALSE}

ffsmodel <- ffs(trainDat[,predictors],
                trainDat$Species_richness,
                method="rf", 
                metric="Rsquared",
                tuneGrid=data.frame("mtry"=2), # no mtry tuning to save time
                ntree=50, # reduce size of the model to save time
                trControl=ctrl_knndm,
                verbose=FALSE)
ffsmodel
ffsmodel$selectedvars
global_validation(ffsmodel)

```

We see that only a few predictors have been selected. Apparently the model can make the same, or even better (indicated by a slightly increased R² performance value) predictions using less variables. All other variables have been selected as either having no relevance or as being counterproductive.

```{r, message = FALSE, warning=FALSE}
plot(ffsmodel)
```


What effect does the new model has on the spatial representation of species richness?

```{r, message = FALSE, warning=FALSE}
prediction_final <- predict(predictors_sp,ffsmodel,na.rm=T)
plot(prediction_final)
```


## Area of Applicability
Still it is required to analyse if the model can be applied to the entire study area of if there are locations that are very different in their predictor properties to what the model has learned from. See more details in the CAST vignette on the Area of applicability and 
[Meyer and Pebesma 2021](https://doi.org/10.1111/2041-210X.13650).

```{r, message = FALSE, warning=FALSE}
AOA <- aoa(predictors_sp,ffsmodel,LPD = TRUE)

plot(AOA$DI)
plot(AOA$AOA)
plot(AOA$LPD) # which areas are well covered?
```

```{r, message = FALSE, warning=FALSE}
plot(prediction_final,main="prediction for the AOA")
plot(AOA$AOA,col=c("grey","transparent"),add=T,legend=FALSE)
legend(-45,12,pch=15,col="grey",legend="NA",bty="n")
```


The figure shows in grey areas that are outside the area of applicability, hence predictions should not be considered for these locations. See tutorial on the AOA in this package for more information.

## Conclusions
To conclude, the tutorial has shown to perform target-oriented (here: spatial) CV on spatial data which is crucial to obtain meaningful validation results. Using the ffs in conjunction with target-oriented validation, variables can be excluded that are counterproductive in view to the target-oriented performance due to misinterpretations by the algorithm. ffs therefore helps to select the ideal set of predictor variables for spatio-temporal prediction tasks and gives objective error estimates. Using the area of applicability we can limit predictions to the environment where the model was enabled to learn about relationships and where we assume that the cross-validation performance applies.

## Final notes
The intention of this tutorial is to describe the basic workflow of using machine learning for spatial mapping and to showcase common issues. Priority is not on modelling species richness of South America in the best possible way but to provide an example for the motivation and functionality of CAST that can run within a few minutes. Hence, only a very small subset of the entire sPlotOpen dataset was used. Keep in mind that due to the small subset the example is not robust and quite different results might be obtained depending on small changes in the settings.

## Further reading

see https://hannameyer.github.io/CAST/
